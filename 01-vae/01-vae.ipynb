{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Variational Autoencoder\n",
    "\n",
    "In a previous notebook we examined a probabilistic model that contains both observed and latent variables, which can be represented as a graphical model:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"graph-latent.png\">\n",
    "</div>\n",
    "\n",
    "For this model the joint probability distribution factorizes as\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}, \\mathbf{z}\\vert\\boldsymbol{\\theta}) = p(\\mathbf{x}\\vert\\mathbf{z},\\boldsymbol{\\theta})p(\\mathbf{z}\\vert\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "For any distribution $q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})$, we can write the marginal log-likelihood as the sum of two terms: the evidence lower bound $\\mathcal{L}(q, \\boldsymbol{\\theta},\\boldsymbol{\\phi})$ and the KL divergence between $q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})$ and the posterior $p(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\theta})$:\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{x}\\vert\\boldsymbol{\\theta}) = \\mathcal{L}(q, \\boldsymbol{\\theta}, \\boldsymbol{\\phi}) + \\text{KL}(q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})\\Vert p(\\mathbf{z\\vert\\mathbf{x}, \\boldsymbol{\\theta}}))\n",
    "$$\n",
    "\n",
    "Our goal is to find the values of $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$ that maximize the marginal log-likelihood. In *variational inference*, we propose $q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})$ as an approximation to the posterior, so that the KL divergence is minimized. The KL divergence is minimized when we maximize the lower bound, defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(q, \\boldsymbol{\\theta}, \\boldsymbol{\\phi}) &= \\mathbb{E}_q[\\log p(\\mathbf{x},\\mathbf{z}\\vert\\boldsymbol{\\theta}) - \\log q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})]\\\\\n",
    "&=\n",
    "\\mathbb{E}_q[\\log p(\\mathbf{x}\\vert\\mathbf{z},\\boldsymbol{\\theta})] - \\text{KL}(q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})\\Vert p(\\mathbf{z}\\vert\\boldsymbol{\\theta}))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can think of $q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})$ as taking the variable $\\mathbf{x}$ and producing a distribution over the latent variable $\\mathbf{z}$. For this reason $q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})$ is also known as the **encoder**: the latent variable $\\mathbf{z}$ acts as a code for the observation $\\mathbf{x}$. The parameters $\\boldsymbol{\\phi}$ are known as the **variational parameters**, because they correspond to the distribution $q$ that we want to use as an approximation to the true posterior.\n",
    "\n",
    "Similarly, we can see that our model for $p(\\mathbf{x}\\vert\\mathbf{z},\\boldsymbol{\\theta})$ does the opposite: given a latent representation, a distribution over the observation is produced. Therefore $p(\\mathbf{x}\\vert\\mathbf{z},\\boldsymbol{\\theta})$ is also known as the **decoder**, which takes the code $\\mathbf{z}$ and reconstructs the observation $\\mathbf{x}$. The parameters $\\boldsymbol{\\theta}$ are known as the **generative parameters**.\n",
    "\n",
    "To maximize the lower bound, we can obtain its gradient with respect to the parameters and then update them in that direction:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}} \\mathcal{L}(q, \\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}}\n",
    "\\left[\n",
    "\\mathbb{E}_q[\\log p(\\mathbf{x}\\vert\\mathbf{z},\\boldsymbol{\\theta})] - \\text{KL}(q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})\\Vert p(\\mathbf{z}\\vert\\boldsymbol{\\theta}))\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "For some cases, the KL divergence can be calculated analytically, as well as its gradient with respect to both the generative and variational parameters. The expectation term can be approximated with a *Monte Carlo estimate*, by taking samples and averaging the result. However, how do we calculate the derivative with respect to $\\boldsymbol{\\phi}$ of a sampling operation from a distribution whose parameter is $\\boldsymbol{\\phi}$ itself?\n",
    "\n",
    "## The reparameterization trick\n",
    "\n",
    "Instead of using $q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})$ to obtain samples of $\\mathbf{z}$, we will introduce an auxiliary random variable $\\boldsymbol{\\epsilon}$ with a corresponding, known distribution $p(\\boldsymbol{\\epsilon})$. We obtain a sample $\\boldsymbol{\\epsilon}$ from this distribution, and then we let $\\mathbf{z}$ be a deterministic, differentiable function of it:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = g(\\mathbf{x},\\boldsymbol{\\epsilon},\\boldsymbol{\\phi})\n",
    "$$\n",
    "\n",
    "Given an appropriate choice of $p(\\boldsymbol{\\epsilon})$ and $g$, $\\mathbf{z}$ would be as if we had sampled from $q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})$, which is what we wanted. The difference now is that the sample $\\mathbf{z}$ was obtained from a differentiable function, and now we can obtain the gradient with respect to $\\boldsymbol{\\phi}$! We can take $L$ samples to obtain the Monte Carlo estimate of the expectation and then differentiate:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}} \\mathcal{L}(q, \\boldsymbol{\\theta}, \\boldsymbol{\\phi}) \\approx \\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}}\\frac{1}{L}\\sum_{i=1}^L \\log p(\\mathbf{x}\\vert\\mathbf{z}^{(i)},\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "with $\\mathbf{z}^{(i)}$ is obtained for each sample of $\\boldsymbol{\\epsilon}$.\n",
    "\n",
    "## The algorithm\n",
    "\n",
    "We now have an algorithm to optimize the lower bound, known as **Autoencoding Variational Bayes** [1]:\n",
    "\n",
    "- Take an observation $\\mathbf{x}$\n",
    "- Take $L$ samples of $\\boldsymbol{\\epsilon}\\sim p(\\boldsymbol{\\epsilon})$ and let $\\mathbf{z}^{(i)}=g(\\mathbf{x},\\boldsymbol{\\epsilon}^{(i)},\\boldsymbol{\\phi})$\n",
    "- Calculate $\\mathbf{g} = \\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}}\\lbrace\\frac{1}{L}\\sum_{i=1}^L \\log p(\\mathbf{x}\\vert\\mathbf{z}^{(i)},\\boldsymbol{\\theta}) - \\text{KL}(q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})\\Vert p(\\mathbf{z}\\vert\\boldsymbol{\\theta}))\\rbrace$\n",
    "- Update $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$ using $\\mathbf{g}$ and an optimizer like Stochastic Gradient Descent or Adam.\n",
    "\n",
    "## A practical example\n",
    "\n",
    "As in the EM example, we will now define a generative model for the MNIST digits dataset. This time, however, we will assume the latent variables to be continuous instead of discrete, so that $\\mathbf{z}\\in\\mathbb{R}^K$ where $K$ is a hyperparameter that indicates the dimension of the latent space. We choose the prior distribution as a Gaussian with zero mean and unit covariance,\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}\\vert\\mathbf{0}, \\mathbf{I})\n",
    "$$\n",
    "\n",
    "Given an observation $\\mathbf{x}$, the approximation of the posterior $p(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\theta})$ (the encoder) will be a Gaussian distribution with diagonal covariance:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{z}\\vert\\mathbf{x}, \\boldsymbol{\\phi}) = \\mathcal{N}(\\mathbf{z}\\vert\\boldsymbol{\\mu}_e,\\text{diag}(\\boldsymbol{\\sigma}_e))\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\mu}_e &= f_{\\phi_\\mu}(\\mathbf{x})\\\\\n",
    "\\log(\\boldsymbol{\\sigma}_e^2) &= f_{\\phi_\\sigma}(\\mathbf{x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $e$ in the subscripts refer to the *encoder*, and $f_{\\phi_\\mu}$ and $f_{\\phi_\\sigma}$ are neural networks with weights $\\boldsymbol{\\phi}_\\mu$ and $\\boldsymbol{\\phi}_\\sigma$, respectively. These parameters form the parameters of the encoder: $\\boldsymbol{\\phi} = \\lbrace\\boldsymbol{\\phi}_\\mu,\\boldsymbol{\\phi}_\\sigma\\rbrace$. The reparameterization trick for this case is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\epsilon}&\\sim\\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\\\\n",
    "\\mathbf{z} &= \\boldsymbol{\\mu}_e + \\boldsymbol{\\sigma}_e\\odot\\boldsymbol{\\epsilon}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "For the prior and approximate posterior that we have defined, the KL divergence is\n",
    "\n",
    "$$\n",
    "\\text{KL}(q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})\\Vert p(\\mathbf{z}\\vert\\boldsymbol{\\theta})) = \\frac{1}{2}\\sum_{i=1}^K(\\sigma_{ei}^2 + \\mu_{ei}^2 - \\log(\\sigma_{ei}^2) - 1)\n",
    "$$\n",
    "\n",
    "Since the pixels in the images are binary, we model an observation $\\mathbf{x}\\in\\mathbb{R}^D$, given the latent variable $\\mathbf{z}$, as a multivariate Bernoulli random variable with mean $\\boldsymbol{\\mu}_d$. This corresponds to the *decoder*:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}\\vert\\mathbf{z},\\boldsymbol{\\theta}) = \\prod_{i=1}^D \\mu_{di}^{x_i}(1-\\mu_{di})^{(1-x_i)}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_d = f_\\theta(\\mathbf{z})\n",
    "$$\n",
    "\n",
    "where $f_\\theta$ is a neural network with weights $\\boldsymbol{\\theta}$. Note that since the output of the decoder models a distribution over a multivariate Bernoulli, we must ensure that its values lie within 0 and 1. We do this with a sigmoid layer at the output.\n",
    "\n",
    "Given this definition of the decoder, we have\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{x}\\vert\\mathbf{z},\\boldsymbol{\\theta}) = \\sum_{i=1}^D x_i\\log\\mu_{di} + (1-x_i)\\log(1-\\mu_{di})\n",
    "$$\n",
    "\n",
    "which corresponds to the binary cross-entropy loss. We now have all the ingredients to implement and train the autoencoder, for which we will use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "class BernoulliVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, enc_units, dec_units):\n",
    "        # Encoder parameters\n",
    "        self.linear_enc = nn.Linear(input_dim, enc_units)\n",
    "        self.enc_mu = nn.Linear(enc_units, latent_dim)\n",
    "        self.enc_logvar = nn.Linear(enc_units, latent_dim)\n",
    "\n",
    "        # Distribution to sample for the reparameterization trick\n",
    "        self.normal_dist = MultivariateNormal(torch.zeros(latent_dim),\n",
    "                                              torch.eye(latent_dim))\n",
    "\n",
    "        # Decoder parameters\n",
    "        self.linear_dec = nn.Linear(latent_dim, dec_units)\n",
    "        self.dec_mu = nn.Linear(dec_units, input_dim)\n",
    "\n",
    "        # Reconstruction loss: binary cross-entropy\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Obtain the parameters of the latent variable distribution\n",
    "        h = F.relu(self.linear_enc(x))\n",
    "        mu_e = self.enc_mu(h)\n",
    "        logvar_e = self.enc_logvar(h)\n",
    "\n",
    "        # Get a latent variable sample with the reparameterization trick\n",
    "        epsilon = self.normal_dist((x.shape[0],))\n",
    "        z = mu_e + torch.sqrt(torch.exp(logvar_e)) * epsilon\n",
    "\n",
    "        return z, mu_e, logvar_e\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Obtain the parameters of the observation distribution\n",
    "        h = F.relu(self.linear_dec(z))\n",
    "        mu_d = F.sigmoid(self.dec_mu(h))\n",
    "\n",
    "        return mu_d\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu_e, logvar_e = self.encode(x)\n",
    "        mu_d = self.decode(z)\n",
    "        cross_entropy = self.criterion(mu_d, input)\n",
    "        kl_div = -0.5*torch.sum((1 + logvar_e) - mu_e**2 - torch.exp(logvar_e))\n",
    "\n",
    "        # Since the optimizer minimizes, we return the negative\n",
    "        # of the lower bound that we need to maximize\n",
    "        return -(cross_entropy - kl_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "<sup>1</sup> In the Expectation Maximization algorithm, we get around this problem by calculating the posterior and simply setting $q(\\mathbf{z}\\vert\\boldsymbol{\\phi}) = p(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\theta})$, which effectively makes the KL divergence equal to zero. However, for some models calculating the posterior is not possible. Furthermore, the EM algorithm calculates updates using the complete dataset, which might not scale up well when we have millions of data points.\n",
    "\n",
    "<sup>2</sup> This last formulation reveals what optimizing the lower bound does. Maximizing the first term finds parameters that given the latent variable $\\mathbf{z}$, assign high probability to the observation $\\mathbf{x}$. We can think of this as a negative reconstruction error, that is, a reconstruction from the latent variable $\\mathbf{z}$ to the observation $\\mathbf{x}$. Maximizing the second term (including the minus sign) minimizes the KL divergence between $q(\\mathbf{z}\\vert\\mathbf{x},\\boldsymbol{\\phi})$ and the prior $p(\\mathbf{z}\\vert\\boldsymbol{\\theta})$, thus acting as a regularizer that enforces a prior structure that we have specified. Briefly stated, we then have\n",
    "\n",
    "$$\n",
    "\\text{lower bound} = -\\text{reconstruction error} - \\text{regularization penalty}\n",
    "$$\n",
    "\n",
    "Therefore, values of $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$ that maximize the lower bound will produce a low reconstruction error and a model that takes into account prior information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
